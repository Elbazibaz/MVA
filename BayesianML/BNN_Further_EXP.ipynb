{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BNN_Further_EXP.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUUfV6Ovw02U",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **Bayesian in Machine Learning**\n",
        "## *MVA - Project*\n",
        "## *Further experiment*\n",
        "## <u>**Authors**</u> : EL BAZ Adrian - FAURÃ‰ Thomas\n",
        "### <u>**Paper**</u> : On the Impact of the Activation Function on Deep Neural Networks Training - **Soufiane Hayou, Arnaud Doucet, Judith Rousseau**\n",
        "\n",
        "---\n",
        "\n",
        "It would interesting to conduct further experiment on bayesian neural network combine with the theorical results of this Paper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxwh0kouVkBS",
        "colab_type": "code",
        "outputId": "f5bfcb73-8a9d-47fa-ff45-af363e925cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "!pip install Pyro4\n",
        "!pip install pyro-ppl\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch.nn.functional import log_softmax\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch import optim\n",
        "import pyro\n",
        "import pyro.optim\n",
        "from pyro.optim import Adam\n",
        "import pyro.infer.elbo\n",
        "from pyro.infer.elbo import ELBO\n",
        "from pyro.distributions import Normal, Categorical\n",
        "\n",
        "\n",
        "class NN(nn.Module):\n",
        "\t\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(NN, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.out(output)\n",
        "        return output\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
        "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
        "        batch_size=128, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
        "                       ),\n",
        "        batch_size=128, shuffle=True)\n",
        "\n",
        "net = NN(28*28, 1024, 10)\n",
        "\n",
        "\n",
        "def model(x_data, y_data):\n",
        "    \n",
        "    fc1w_prior = Normal(loc=torch.zeros_like(net.fc1.weight), scale=torch.ones_like(net.fc1.weight))\n",
        "    fc1b_prior = Normal(loc=torch.zeros_like(net.fc1.bias), scale=torch.ones_like(net.fc1.bias))\n",
        "    \n",
        "    outw_prior = Normal(loc=torch.zeros_like(net.out.weight), scale=torch.ones_like(net.out.weight))\n",
        "    outb_prior = Normal(loc=torch.zeros_like(net.out.bias), scale=torch.ones_like(net.out.bias))\n",
        "    \n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    \n",
        "    # lift module parameters to random variables sampled from the priors\n",
        "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
        "    # sample a regressor (which also samples w and b)\n",
        "    lifted_reg_model = lifted_module()\n",
        "    \n",
        "    lhat = log_softmax(lifted_reg_model(x_data))\n",
        "    \n",
        "    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)\n",
        "\n",
        "softplus = torch.nn.functional.softplus\n",
        "\n",
        "def guide(x_data, y_data):\n",
        "    \n",
        "    # First layer weight distribution priors\n",
        "    fc1w_mu = torch.randn_like(net.fc1.weight)\n",
        "    fc1w_sigma = torch.randn_like(net.fc1.weight)\n",
        "    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n",
        "    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n",
        "    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n",
        "\n",
        "    # First layer bias distribution priors\n",
        "    fc1b_mu = torch.randn_like(net.fc1.bias)\n",
        "    fc1b_sigma = torch.randn_like(net.fc1.bias)\n",
        "    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n",
        "    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n",
        "    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n",
        "\n",
        "    # Output layer weight distribution priors\n",
        "    outw_mu = torch.randn_like(net.out.weight)\n",
        "    outw_sigma = torch.randn_like(net.out.weight)\n",
        "    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n",
        "    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n",
        "    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param)#.independent(1)\n",
        "\n",
        "    # Output layer bias distribution priors\n",
        "    outb_mu = torch.randn_like(net.out.bias)\n",
        "    outb_sigma = torch.randn_like(net.out.bias)\n",
        "    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n",
        "    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n",
        "    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n",
        "    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n",
        "    \n",
        "    lifted_module = pyro.random_module(\"module\", net, priors)\n",
        "    \n",
        "    return lifted_module()\n",
        "\n",
        "\n",
        "opti = Adam({'lr': 0.01})\n",
        "svi = pyro.infer.SVI(model, guide, opti, loss = pyro.infer.Trace_ELBO())\n",
        "\n",
        "num_iterations = 5\n",
        "loss = 0\n",
        "\n",
        "for j in range(num_iterations):\n",
        "    loss = 0\n",
        "    for batch_id, data in enumerate(train_loader):\n",
        "        # calculate the loss and take a gradient step\n",
        "        loss += svi.step(data[0].view(-1,28*28), data[1])\n",
        "    normalizer_train = len(train_loader.dataset)\n",
        "    total_epoch_loss_train = loss / normalizer_train\n",
        "    \n",
        "    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "num_samples = 10\n",
        "def predict(x):\n",
        "    sampled_models = [guide(None, None) for _ in range(num_samples)]\n",
        "    yhats = [model(x).data for model in sampled_models]\n",
        "    mean = torch.mean(torch.stack(yhats), 0)\n",
        "    return np.argmax(mean.numpy(), axis=1)\n",
        "\n",
        "print('Prediction when network is forced to predict')\n",
        "correct = 0\n",
        "total = 0\n",
        "for j, data in enumerate(test_loader):\n",
        "    images, labels = data\n",
        "    predicted = predict(images.view(-1,28*28))\n",
        "    total += labels.size(0)\n",
        "    correct += (np.array(predicted) == np.array(labels)).sum().item()\n",
        "print(\"accuracy: %d %%\" % (100 * correct / total))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Pyro4 in /usr/local/lib/python3.6/dist-packages (4.79)\n",
            "Requirement already satisfied: serpent>=1.27; python_version >= \"3.2\" in /usr/local/lib/python3.6/dist-packages (from Pyro4) (1.30.2)\n",
            "Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.43.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.17.5)\n",
            "Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.1.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.4.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n",
            "  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch  0  Loss  92.82690940818786\n",
            "Epoch  1  Loss  88.69328067015012\n",
            "Epoch  2  Loss  87.38730148100854\n",
            "Epoch  3  Loss  86.96898370869954\n",
            "Epoch  4  Loss  86.18120519477526\n",
            "Prediction when network is forced to predict\n",
            "accuracy: 90 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}