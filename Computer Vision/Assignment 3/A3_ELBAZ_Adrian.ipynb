{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A3_ELBAZ_Adrian.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAnFfO1S0xVz",
        "colab_type": "text"
      },
      "source": [
        "# Imports and loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-AhtzKk1RQe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Getting the datasets\n",
        "!wget https://www.di.ens.fr/willow/teaching/recvis18/assignment3/bird_dataset.zip \n",
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/bird_dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtUmhHgy04ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import PIL.Image as Image\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import glob\n",
        "from __future__ import print_function, division\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "# Import the Model Zoo package to get Inceptionresnetv2 architecture\n",
        "from cnn_finetune import make_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMfkhmvl1E9F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train_images': transforms.Compose([\n",
        "        transforms.Resize((400,400)),\n",
        "        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.05, hue=0),transforms.RandomAffine(degrees=(-45,45), shear=(20,20), resample=False),transforms.RandomHorizontalFlip()], p=0.75),\n",
        "        torchvision.transforms.RandomResizedCrop((299,299)),\n",
        "        torchvision.transforms.RandomPerspective(distortion_scale=0.3, p=0.5, interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "    ]),\n",
        "    'val_images': transforms.Compose([\n",
        "        transforms.Resize((400,400)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "#dt = {'train_images' : transforms.Compose([transforms.ToTensor()]), 'val_images': transforms.Compose([transforms.ToTensor()])}\n",
        "data_dir = 'bird_dataset/'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),data_transforms[x]) for x in ['train_images', 'val_images']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                             shuffle=True, num_workers=1)\n",
        "              for x in ['train_images', 'val_images']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train_images', 'val_images']}\n",
        "class_names = image_datasets['train_images'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCx1U3eU2DLo",
        "colab_type": "text"
      },
      "source": [
        "# Creation of a new dataset with Mask RCNN\n",
        "---\n",
        "> The new dataset is called **birds_cropped**. It consists to the same dataset as the original but with cropped images obtained by applying the Mask-RCNN algorithm to detect birds.\n",
        ">\n",
        "> If a no birds is detected, we keep the **original** image.\n",
        "> \n",
        "> If multiple birds are detected, we take the bounding box that has the **highest** score.\n",
        ">\n",
        ">\n",
        ">The orginal code comes from the python demo in this link :\n",
        ">https://github.com/matterport/Mask_RCNN/tree/master/mrcnn\n",
        ">\n",
        "> If you want to reproduce the results, you should get the following file from the GitHub repo :\n",
        "* utils.py\n",
        "* model.py\n",
        "* visualize.py\n",
        "* parallel_model.py\n",
        "* config.py\n",
        "* \\_\\_init\\_\\_.py\n",
        "\n",
        ">\n",
        "> Also, imports from these files should be modified (only the imports between these files in the beginning of the code) since there will be in the same repository(colab) and not in mrcnn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH1Ekw_92yIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Load the model : COCO \n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "import skimage.io\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "os.mkdir(\"coco\")\n",
        "# Root directory of the project\n",
        "ROOT_DIR = os.path.abspath(\"coco\")\n",
        "\n",
        "# Import Mask RCNN\n",
        "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
        "import utils\n",
        "import model as modellib\n",
        "import visualize\n",
        "# Import COCO config\n",
        "#sys.path.append(os.path.join(ROOT_DIR, \"samples/coco/\"))  # To find local version\n",
        "import coco\n",
        "\n",
        "%matplotlib inline \n",
        "\n",
        "# Directory to save logs and trained model\n",
        "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
        "\n",
        "# Local path to trained weights file\n",
        "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
        "# Download COCO trained weights from Releases if needed\n",
        "if not os.path.exists(COCO_MODEL_PATH):\n",
        "    utils.download_trained_weights(COCO_MODEL_PATH)\n",
        "\n",
        "# Directory of images to run detection on\n",
        "IMAGE_DIR = \"/content/birds_dataset/test_images/mistery_category\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fJ6QXeA4GjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from config import Config\n",
        "class CocoConfig(Config):\n",
        "    \"\"\"Configuration for training on MS COCO.\n",
        "    Derives from the base Config class and overrides values specific\n",
        "    to the COCO dataset.\n",
        "    \"\"\"\n",
        "    # Give the configuration a recognizable name\n",
        "    NAME = \"coco\"\n",
        "\n",
        "    # We use a GPU with 12GB memory, which can fit two images.\n",
        "    # Adjust down if you use a smaller GPU.\n",
        "    IMAGES_PER_GPU = 2\n",
        "\n",
        "    # Uncomment to train on 8 GPUs (default is 1)\n",
        "    # GPU_COUNT = 8\n",
        "\n",
        "    # Number of classes (including background)\n",
        "    NUM_CLASSES = 1 + 80  # COCO has 80 classes\n",
        "\n",
        "\n",
        "# Import Mask RCNN\n",
        "#sys.path.append(ROOT_DIR)\n",
        "#from coco import CocoConfig\n",
        "class InferenceConfig(CocoConfig):\n",
        "    # Set batch size to 1 since we'll be running inference on\n",
        "    # one image at a time. Batch size = GPU_COUNT * IMAGES_PER_GPU\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1\n",
        "\n",
        "config = InferenceConfig()\n",
        "config.display()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJqiw4yk4Hp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create model object in inference mode.\n",
        "model_mrcnn = modellib.MaskRCNN(mode=\"inference\", model_dir=MODEL_DIR, config=config)\n",
        "\n",
        "# Load weights trained on MS-COCO\n",
        "model_mrcnn.load_weights(COCO_MODEL_PATH, by_name=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5112g2y4N8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customed function to create directory (works on Colab)\n",
        "def verif_roi_results(r):\n",
        "  \"\"\" either return the index of the highest scoring bird or -1 if no birds is detected\"\"\"\n",
        "  ## Get index of ROI of birds\n",
        "  idx = [i for i in range(len(r['class_ids'])) if r['class_ids'][i] == 15]\n",
        "  if (len(idx) == 0):\n",
        "    return -1\n",
        "  else : \n",
        "    ## get the highest score idx for birds \n",
        "    highest_score_index = np.argmax(r['scores'][idx])\n",
        "    hscore_i = idx[highest_score_index]\n",
        "    return hscore_i\n",
        "\n",
        "def extract_maskrcnn(model,type_dataset = \"train\"):\n",
        "  \"\"\"Allows to extract the birds images from original images using mask R-CNN \"\"\"\n",
        "  os.chdir(\"/content\")\n",
        "  if not os.path.exists(\"birds_cropped\"):\n",
        "    os.mkdir(\"birds_cropped\")\n",
        "    print(\"Folder birds_cropped created\")\n",
        "  else :\n",
        "    os.chdir(\"/content\")\n",
        "  # Choose mod  \n",
        "  if (type_dataset == 'test'):\n",
        "\n",
        "    IMAGE_DIR = \"/content/bird_dataset/test_images/mistery_category\"\n",
        "    file_names = next(os.walk(IMAGE_DIR))[2]\n",
        "    os.chdir(\"birds_cropped\")\n",
        "    os.mkdir(\"test_images\")\n",
        "    os.chdir(\"test_images\")\n",
        "    os.mkdir(\"mistery_category\")\n",
        "    os.chdir(\"mistery_category\")\n",
        "    for p in file_names :\n",
        "      image = skimage.io.imread(os.path.join(IMAGE_DIR, p))\n",
        "      # Run detection\n",
        "      \n",
        "      results = model.detect([image])\n",
        "      r = results[0]\n",
        "      ## Processing if multiple object in the image\n",
        "      idx = verif_roi_results(r)\n",
        "      \n",
        "        \n",
        "      # Get new image from bbox outputs\n",
        "      im = Image.open(os.path.join(IMAGE_DIR, p))\n",
        "      if (idx == -1):\n",
        "        im.save(p,'PNG')\n",
        "      else : \n",
        "        newimg = torchvision.transforms.functional.crop(im, r['rois'][idx][0],r['rois'][idx][1] , r['rois'][idx][2] - r['rois'][idx][0],r['rois'][idx][3] - r['rois'][idx][1])\n",
        "      # save image in the new repo\n",
        "        im.close()\n",
        "        newimg.save(p,\"PNG\")\n",
        "  else :\n",
        "    ## Train images\n",
        "    IMAGE_DIR = \"/content/bird_dataset/train_images\"\n",
        "    path_list = sorted(glob.glob(IMAGE_DIR +\"/*\"))\n",
        "    name_directory = sorted([x[0][35:] for x in os.walk(IMAGE_DIR)][1:])\n",
        "\n",
        "    os.chdir(\"birds_cropped\")\n",
        "    os.mkdir(\"train_images\")\n",
        "    os.chdir(\"train_images\")\n",
        "    for cat_path, name_cat in zip(path_list,name_directory) : \n",
        "      file_names = next(os.walk(cat_path))[2]\n",
        "      os.chdir(\"/content/birds_cropped/train_images\")\n",
        "      os.mkdir(name_cat)\n",
        "      os.chdir(name_cat)\n",
        "      for p in file_names : \n",
        "        try :\n",
        "          image = skimage.io.imread(os.path.join(cat_path, p))\n",
        "          # Run detection\n",
        "          \n",
        "          results = model.detect([image])\n",
        "          r = results[0]\n",
        "          ## Processing if multiple object in the image\n",
        "          idx = verif_roi_results(r)\n",
        "          # Get new image from bbox outputs\n",
        "          im = Image.open(os.path.join(cat_path, p))\n",
        "\n",
        "          newimg = torchvision.transforms.functional.crop(im, r['rois'][idx][0],r['rois'][idx][1] , r['rois'][idx][2] - r['rois'][idx][0],r['rois'][idx][3] - r['rois'][idx][1])\n",
        "          # save image in the new repo\n",
        "          im.close()\n",
        "          newimg.save(p,\"PNG\")\n",
        "        except :\n",
        "          pass\n",
        "    ## Val images \n",
        "    IMAGE_DIR = \"/content/bird_dataset/val_images\"\n",
        "    path_list = sorted(glob.glob(IMAGE_DIR +\"/*\"))\n",
        "    name_directory = sorted([x[0][33:] for x in os.walk(IMAGE_DIR)][1:])\n",
        "\n",
        "    os.chdir(\"/content/birds_cropped\")\n",
        "    os.mkdir(\"val_images\")\n",
        "    os.chdir(\"val_images\")\n",
        "    for cat_path, name_cat in zip(path_list,name_directory) : \n",
        "      file_names = next(os.walk(cat_path))[2]\n",
        "      os.chdir(\"/content/birds_cropped/val_images\")\n",
        "\n",
        "      os.mkdir(name_cat)\n",
        "      os.chdir(name_cat)\n",
        "      for p in file_names : \n",
        "        try :\n",
        "          image = skimage.io.imread(os.path.join(cat_path, p))\n",
        "          # Run detection\n",
        "          results = model.detect([image])\n",
        "          r = results[0]\n",
        "          ## Processing if multiple object in the image\n",
        "          idx = verif_roi_results(r)\n",
        "          # Get new image from bbox outputs\n",
        "          im = Image.open(os.path.join(cat_path, p))\n",
        "\n",
        "          newimg = torchvision.transforms.functional.crop(im, r['rois'][idx][0],r['rois'][idx][1] , r['rois'][idx][2] - r['rois'][idx][0],r['rois'][idx][3] - r['rois'][idx][1])\n",
        "          # save image in the new repo\n",
        "          im.close()\n",
        "          newimg.save(p,\"PNG\")\n",
        "        except : \n",
        "          pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lb2xn6jK4ZSc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Extract birds from val and train images \n",
        "extract_maskrcnn(model)\n",
        "os.chdir('/content')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IRAba754bk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Extract birds from test images\n",
        "extract_maskrcnn(model,type_dataset = 'test')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpqMLAQ45Gny",
        "colab_type": "text"
      },
      "source": [
        "# Training models\n",
        "---\n",
        "> We got the Inception Resnetv2 from cnn_fintuned library which is available on **Model Zoo**.\n",
        ">\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhA_1gEM5VqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train_images', 'val_images']:\n",
        "            if phase == 'train_images':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train_images'):\n",
        "                  ## Train inception \n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    #loss2 = criterion(aux,labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train_images':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            #if phase == 'train_images':\n",
        "            #    scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train_images':\n",
        "                  scheduler.step(epoch_loss)\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val_images' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMmTlRA97XR8",
        "colab_type": "text"
      },
      "source": [
        "## Inception Resnet v2 trained on full images and then cropped images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAg3NLeA6Gnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Custom classifier to add at the end of a pre-trained architecture\n",
        "def make_classifier(in_features, num_classes):\n",
        "    return nn.Sequential(\n",
        "        nn.Linear(in_features, 4096),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Linear(4096, num_classes),\n",
        "    )\n",
        "\n",
        "model_i = make_model('inceptionresnetv2', num_classes=20, pretrained=True, input_size=(400, 400), classifier_factory=make_classifier)\n",
        "\n",
        "## Then we need to freeze layers at the beginning (the first ones as they play the role of feature extractors)\n",
        "for name, child in model_i.named_children():\n",
        "        if name == '_features':\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "for name, child in model_i.named_children():\n",
        "        if name == '_features':\n",
        "          for n in [9,10,11,12,13,14]:\n",
        "            for param in child[n].parameters() :\n",
        "                param.requires_grad =True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpjJ-lu-5fss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_i = model_i.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction= 'mean')\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_i.parameters(), lr=0.01, momentum=0.9)\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "model_i = train_model(model_i, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKsf9YBM7Lls",
        "colab_type": "text"
      },
      "source": [
        "## Resnet 152 trained on full images\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_oXoEx353zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_resnet152_fi = models.resnet152(pretrained=True)\n",
        "num_ftrs = model_resnet152_fi.fc.in_features\n",
        "lay_unfreeze = ['layer4','layer3']\n",
        "for name, child in model_resnet152_fi.named_children():\n",
        "        if name in lay_unfreeze:\n",
        "          if(name =='layer3'):\n",
        "            list_bottleneck =[i for i in range(17,36)]\n",
        "            for n in range(0,36):\n",
        "              if(n>21):\n",
        "                for param in child[n].parameters():\n",
        "                  param.requires_grad=True\n",
        "              else :\n",
        "                for param in child[n].parameters():\n",
        "                  param.requires_grad=False\n",
        "              \n",
        "            \n",
        "          elif(name =='layer4') :\n",
        "              print(name + 'layer4 has been unfrozen.')\n",
        "              for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_resnet152_fi.fc = nn.Linear(num_ftrs, 20)\n",
        "\n",
        "model_resnet152_fi = model_resnet152_fi.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction= 'mean')\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_resnet152_fi.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n",
        "\n",
        "model_resnet152_fi = train_model(model_resnet152_fi, criterion, optimizer_ft, exp_lr_scheduler,num_epochs=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pr4-CktM7t_X",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lo6zmLOi_0Rn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "        transforms.RandomApply([transforms.ColorJitter(brightness=0.4, contrast=0.2, saturation=0.2, hue=0),transforms.RandomAffine(degrees=(-45,45), shear=(20,20), resample=False),transforms.RandomHorizontalFlip()], p=0.75),\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_13Xg1vC-0Q",
        "colab_type": "text"
      },
      "source": [
        "## Resnet 152 trained on birds_cropped\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9FsOY1KDOJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_transforms = {\n",
        "    'train_images': transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.1, saturation=0.05, hue=0),transforms.RandomAffine(degrees=(-45,45), shear=(20,20), resample=False),transforms.RandomHorizontalFlip()], p=0.75),\n",
        "        torchvision.transforms.RandomResizedCrop((299,299)),\n",
        "        torchvision.transforms.RandomPerspective(distortion_scale=0.3, p=0.5, interpolation=3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'val_images': transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "#dt = {'train_images' : transforms.Compose([transforms.ToTensor()]), 'val_images': transforms.Compose([transforms.ToTensor()])}\n",
        "data_dir = 'birds_cropped/'\n",
        "\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),data_transforms[x]) for x in ['train_images', 'val_images']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=64,\n",
        "                                             shuffle=True, num_workers=1)\n",
        "              for x in ['train_images', 'val_images']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train_images', 'val_images']}\n",
        "class_names = image_datasets['train_images'].classes\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "################################################################################\n",
        "\n",
        "\n",
        "model_ft = models.resnet152(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "lay_unfreeze = ['layer4','layer3']\n",
        "for name, child in model_ft.named_children():\n",
        "        if name in lay_unfreeze:\n",
        "          if(name =='layer3'):\n",
        "            list_bottleneck =[i for i in range(17,36)]\n",
        "            for n in range(0,36):\n",
        "              if(n>21):\n",
        "                for param in child[n].parameters():\n",
        "                  param.requires_grad=True\n",
        "              else :\n",
        "                for param in child[n].parameters():\n",
        "                  param.requires_grad=False\n",
        "              \n",
        "            \n",
        "          elif(name =='layer4') :\n",
        "              print(name + 'layer4 has been unfrozen.')\n",
        "              for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        else:\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 20)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(reduction= 'mean')\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wpZH3vh1t2d",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "---\n",
        "> Here we implemented our **emsemble method** to ouput a csv file that can be uploaded in Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUB8jy0c1v9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Different transforms due to different image size\n",
        "data_transforms_2 = {\n",
        "    'val_images': transforms.Compose([\n",
        "        transforms.Resize((299,299)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}\n",
        "\n",
        "data_transforms_1 = {\n",
        "    'val_images': transforms.Compose([\n",
        "        transforms.Resize((400,400)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSPUjLoE1-hM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## The test images are first processed to detect birds in it. If a no bird is detected, then we keep the original image.\n",
        "test_dir_2 = '/content/birds_cropped' + '/test_images/mistery_category'\n",
        "test_dir_1 = '/content/bird_dataset' + '/test_images/mistery_category'\n",
        "\n",
        "def pil_loader(path):\n",
        "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
        "    with open(path, 'rb') as f:\n",
        "        with Image.open(f) as img:\n",
        "            return img.convert('RGB')\n",
        "\n",
        "## Models are in eval mode\n",
        "model_resnet152fi.eval()\n",
        "model_i.eval()\n",
        "model_ft.eval()\n",
        "#model_inception.eval()\n",
        "\n",
        "## Create the output csv file\n",
        "output_file = open(\"kaggle_ensemble3.csv\", \"w\")\n",
        "output_file.write(\"Id,Category\\n\")\n",
        "## Images names in the 2 dataset are identical, only their paths is different\n",
        "for f in tqdm(os.listdir(test_dir_1)):\n",
        "    if 'jpg' in f:\n",
        "        data = data_transforms_1[\"val_images\"](pil_loader(test_dir_1 + '/' + f))\n",
        "        data2 = data_transforms_2[\"val_images\"](pil_loader(test_dir_2 + '/' + f))\n",
        "        data = data.view(1, data.size(0), data.size(1), data.size(2))\n",
        "        data2 = data2.view(1,data2.size(0), data2.size(1), data2.size(2))\n",
        "        data2 = data2.to(device)\n",
        "        data = data.to(device)\n",
        "        output1 = model_i(data2)\n",
        "        output2 = model_ft(data2)\n",
        "        output3 = model_resnet152fi(data)\n",
        "        \n",
        "        output1 = torch.nn.functional.softmax(output1)\n",
        "        output2 = torch.nn.functional.softmax(output2)\n",
        "        output3 = torch.nn.functional.softmax(output3)\n",
        "        \n",
        "        w_mean = output1+output2+output3\n",
        "        pred = w_mean.data.max(1,keepdim = True)[1]\n",
        "        \n",
        "        output_file.write(\"%s,%d\\n\" % (f[:-4], pred))\n",
        "\n",
        "\n",
        "output_file.close()\n",
        "\n",
        "print(\"Succesfully wrote \" + \"kaggle_ensemble3.csv\" + ', you can upload this file to the kaggle competition website')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}